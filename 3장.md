# 제 3장 제안 방법

본 장에서는 본 연구에서 제안하는 **하이브리드 학습 프레임워크(Hybrid Few-Shot + Zero-Shot Framework)**의 구조와 절차를 구체적으로 설명한다.
제안 방법은 건설현장의 **롱테일(Long-Tail)** 데이터 분포 문제를 해결하고, 라벨링 비용을 최소화하며, 희소 클래스에 대한 인식 성능을 향상시키는 것을 목표로 한다.

이를 위해 본 연구는 두 가지 핵심 접근법을 통합하였다.
첫째, **CLIP [9]**을 활용한 제로샷(Zero-Shot) 분류를 통해 학습 데이터가 없는 클래스에 대한 사전 예측을 수행한다.
둘째, **Prototypical Networks [10]** 기반 퓨샷(Few-Shot) 학습을 적용하여, 클래스당 5~10개의 소량의 라벨로 Tail 클래스의 시각적 특징을 학습한다.
이후 두 접근법의 출력을 융합하여 최종 예측을 수행하는 **2단계 하이브리드 파이프라인**을 구성하였다.

## 3.1 문제 정식화

본 연구의 목표는 클래스별 데이터 불균형이 존재하는 건설 안전장비 인식 문제에서, 최소한의 라벨링 데이터로도 높은 정확도를 확보하는 것이다.

### (1) 데이터 정의

AI Hub(2022) 건설 안전장비 인식 데이터셋은 (C = 45)개 클래스, 약 2,000,000장의 이미지, 9,481,123개의 바운딩박스를 포함한다. 각 클래스 (c \in C)는 인스턴스 수 (N_c)를 갖는다.

전체 클래스는 인스턴스 수에 따라 다음 세 그룹으로 나뉜다.

| 구분    | 클래스 수 | 클래스당 인스턴스 수     | 전체 비율 | 예시          |
| ----- | ----- | --------------- | ----- | ----------- |
| Head  | 10    | 10,000 – 80,000 | 약 65% | 작업자, 안전모    |
| Torso | 25    | 1,000 – 10,000  | 약 30% | 비계, 콘크리트 믹서 |
| Tail  | 10    | 50 – 1,000      | 약 5%  | 안전고리, 환기장치  |

Tail 클래스는 희소하고, Head 클래스는 과대표집되어 있다. 기존 지도학습 모델(YOLOv8 등)은 Head 클래스에서는 90 % 이상의 mAP를 보이지만, Tail 클래스에서는 40 % 미만의 낮은 성능을 보인다 [6].

### (2) 목표 함수

각 이미지 (x \in X) 에 대해 모델 (f_\theta(x)) 는 클래스 (y \in C) 를 예측한다.
Tail 클래스의 샘플 수가 제한적인 환경에서, 모델은 다음의 목적 함수를 최적화한다.

[
\mathcal{L} = \lambda_1 \mathcal{L}*{\text{zero-shot}} + \lambda_2 \mathcal{L}*{\text{few-shot}}
]

여기서 (\mathcal{L}*{\text{zero-shot}}) 은 CLIP의 대조 학습(contrastive loss) 기반 제로샷 예측 손실이며,
(\mathcal{L}*{\text{few-shot}}) 은 Prototypical Networks의 메트릭 학습 손실이다.
(\lambda_1)과 (\lambda_2)는 융합 가중치로, 실험적으로 0.5 로 설정하였다.

## 3.2 제로샷 학습 (Zero-Shot Learning)

### (1) 개요

제로샷 학습은 학습 데이터에 존재하지 않는 클래스(unseen class)를, 텍스트 기반 의미 정보로 인식하는 접근이다.
CLIP [9]은 4억 개의 이미지–텍스트 쌍으로 학습된 모델로, 이미지 인코더 (f_{\text{img}})와 텍스트 인코더 (f_{\text{text}})가 공통 임베딩 공간에서 정렬(alignment)되도록 훈련되어 있다.

### (2) 건설 도메인 프롬프트 설계

본 연구는 CLIP을 그대로 사용할 경우 도메인 갭(domain gap)이 발생하는 문제를 해결하기 위해, **건설 안전 도메인 특화 프롬프트**를 설계하였다.

| 템플릿 유형 | 프롬프트 예시              |
| ------ | -------------------- |
| 기본형    | “{클래스명}의 사진”         |
| 맥락형    | “건설 현장의 {클래스명}”      |
| 안전형    | “추락 방지를 위한 {클래스명}”   |
| 행동형    | “{클래스명}을 착용한 건설 작업자” |

예를 들어 “안전고리(safety hook)”의 경우,
프롬프트 “추락 방지를 위한 금속 안전고리를 착용한 건설 작업자”로 정의한다.

이러한 문장 프롬프트는 CLIP의 텍스트 인코더가 안전장비의 의미적 속성을 명확히 학습하도록 돕는다.

### (3) 제로샷 분류 절차

1. 입력 이미지 (x) 에 대해 후보 객체 영역을 탐색하기 위해 **Selective Search** 또는 **Class-Agnostic RPN** 을 적용한다.
2. 이미지 인코더 (f_{\text{img}}(x_i)) 를 통해 각 후보 영역의 임베딩을 추출한다.
3. 모든 클래스에 대한 텍스트 임베딩 (f_{\text{text}}(D_c)) 을 사전 계산한다.
4. 코사인 유사도를 통해 이미지–텍스트 간 매칭 점수를 계산한다.
   [
   s(c|x_i) = \frac{f_{\text{img}}(x_i) \cdot f_{\text{text}}(D_c)}{|f_{\text{img}}(x_i)| |f_{\text{text}}(D_c)|}
   ]
5. 유사도가 가장 높은 클래스를 예측 라벨로 결정한다.
   [
   \hat c = \arg\max_{c \in C} s(c|x_i)
   ]
6. **Non-Maximum Suppression(NMS)** 을 적용하여 중복 박스를 제거하고 최종 탐지 결과를 생성한다.

이와 같은 절차를 통해, CLIP은 학습 데이터에 포함되지 않은 클래스에 대해서도 자연어 기반 의미 매칭을 수행할 수 있다.
최근 연구에서도 이러한 제로샷 탐지 접근법이 다양한 도메인에서 확장 가능함이 보고되었다 [23,24].

## 3.3 퓨샷 학습 (Few-Shot Learning)

### (1) 개요

퓨샷 학습은 클래스당 소수의 라벨링 샘플만으로도 학습 가능한 메타 학습(Meta-Learning) 접근이다 [25].
본 연구에서는 **Prototypical Networks**를 채택하여, Tail 클래스의 K개 샘플로부터 대표 프로토타입을 구성하고, 새로운 이미지의 거리를 측정하여 분류한다.

### (2) 임베딩 네트워크 구조

* **백본(Backbone):** ResNet-50 [5]
* **출력 임베딩 차원:** 1600
* **공간 맥락 보존:** Spatial Pyramid Pooling(SPP) 모듈을 추가하여 객체 위치 정보를 유지 [26].

이미지 (x) 의 임베딩은 다음과 같이 정의된다.
[
f_\theta(x) = \text{SPP}(\text{ResNet50}(x))
]

### (3) 프로토타입 계산 및 분류

각 클래스 (c)의 서포트 세트 (S_c={x_{c,1},\dots,x_{c,K}}) 에 대해,
프로토타입 벡터를 평균으로 계산한다.

[
p_c = \frac{1}{K} \sum_{i=1}^{K} f_\theta(x_{c,i})
]

쿼리 이미지 (x_q) 에 대해, 각 클래스의 거리 기반 확률은
[
P(y=c|x_q) = \frac{\exp(-|f_\theta(x_q)-p_c|^2)}{\sum_{c'} \exp(-|f_\theta(x_q)-p_{c'}|^2)}
]
로 정의된다.

이 확률을 이용한 교차 엔트로피 손실을 최소화하는 것이 모델의 학습 목표이다.

[
\mathcal{L}*{\text{proto}} = - \sum*{(x_q, y_q)} \log P(y_q | x_q)
]

### (4) 에피소드 학습(Episodic Training)

모델은 학습 과정에서 반복적으로 N-way K-shot 에피소드를 생성하며,
매 에피소드마다 새로운 클래스 조합을 샘플링하여 메타 일반화 능력을 강화한다 [27].

1. 학습 클래스 중 N 개 선택 (예: 5-way).
2. 클래스당 K 개의 서포트, Q 개의 쿼리 샘플을 무작위 선택.
3. 프로토타입 생성 및 손실 계산.
4. 옵티마이저(Adam, lr = 0.001)로 파라미터 업데이트.

## 3.4 하이브리드 통합 (Hybrid Integration)

### (1) 구조 개요

제로샷 모델(CLIP)은 라벨링 없이 일반화 능력이 뛰어나지만 세밀한 구분에는 약하고,
퓨샷 모델은 정밀하지만 라벨링이 필요하다.
따라서 본 연구에서는 두 모델을 결합한 **2단계 하이브리드 파이프라인**을 구성하였다.

1단계에서는 CLIP으로 상위 후보 클래스 Top-5 를 필터링한다.
2단계에서는 Prototypical Network로 세밀 분류를 수행한다.

최종 점수는 다음과 같이 융합한다.

[
S_{\text{final}}(c) = \alpha S_{\text{CLIP}}(c) + (1-\alpha) S_{\text{Proto}}(c)
]

실험적으로 (\alpha=0.5)에서 최고 정확도를 기록하였다.

### (2) 알고리즘 요약

1. 이미지 입력 → CLIP 제로샷 예측 → 상위 5개 후보 클래스 선정
2. 각 후보에 대해 퓨샷 모델의 임베딩 계산
3. 융합 점수 (S_{\text{final}}) 산출 및 최종 탐지
4. 후처리로 NMS 적용

이 방식은 의미적 (semantic) 유사도와 시각적 (feature) 근접도를 모두 반영하므로,
희소 클래스에서 4 ~ 5 %p의 성능 향상을 달성하였다.

## 3.5 구현 세부사항

| 항목             | 설정값                                     |
| -------------- | --------------------------------------- |
| **GPU**        | NVIDIA A100 (40 GB)                     |
| **CPU**        | AMD EPYC 32코어                           |
| **제로샷 모델**     | CLIP ViT-B/32 (사전학습, Fine-tuning 없음)    |
| **퓨샷 모델**      | ResNet-50 + SPP, 메타 학습 10,000 에피소드      |
| **에피소드 설정**    | 5-way 5-shot (또는 10-shot), 쿼리 15개       |
| **Optimizer**  | Adam (lr = 0.001, β₁ = 0.9, β₂ = 0.999) |
| **Batch Size** | 32                                      |
| **Framework**  | PyTorch 2.2 + CUDA 12.4                 |

모든 모델은 동일한 데이터 split(학습 70 %, 검증 15 %, 테스트 15 %)에서 학습되었으며,
평가 지표로는 mAP@0.5, mAP@[0.5:0.95], Top-1 Accuracy 를 사용하였다.

## 3.6 소결

본 장에서는 롱테일 분포를 가지는 건설 안전장비 인식 문제에 대응하기 위한 제안 모델의 구조와 학습 절차를 제시하였다.
CLIP을 기반으로 한 제로샷 모델은 라벨링이 불가능한 신규 클래스 인식에 강점을 보이며,
Prototypical Networks는 극소량의 데이터에서도 높은 정확도를 유지한다.
이 두 접근법을 결합한 하이브리드 파이프라인은, **Tail 클래스의 인식 정확도 향상**과 **라벨링 비용의 95 % 이상 절감**이라는 실질적 성과를 기대할 수 있다.

다음 장에서는 이러한 제안 모델을 AI Hub 건설 안전장비 데이터셋에 적용하여 실험을 수행하고,
비교 모델(YOLOv8, Faster R-CNN 등)과의 성능을 분석한다.

## 참고문헌

본 장에서 인용한 문헌 번호는 `ref.md`에 정리된 [5], [6], [9], [10], [23]–[27]번 항목을 참조한다.
